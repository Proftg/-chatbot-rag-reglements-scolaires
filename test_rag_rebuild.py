#!/usr/bin/env python3
"""
Script de test pour reconstruire l'index RAG avec les PDFs locaux
"""
import os
from pathlib import Path
from pypdf import PdfReader
from langchain_core.documents import Document
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain_community.embeddings import SentenceTransformerEmbeddings
import re

def preprocess_text(text: str) -> str:
    """Nettoie le texte extrait."""
    # Suppression des m√©tadonn√©es web
    text = re.sub(r'(likes|comments|add comment|share|skip to content)', '', text, flags=re.IGNORECASE)
    # Normalisation des espaces
    text = re.sub(r'\s+', ' ', text)
    # Normalisation des sauts de ligne
    text = re.sub(r'\n{3,}', '\n\n', text)
    return text.strip()

def extract_pdfs():
    """Extrait et charge tous les PDFs depuis le dossier R√©glements."""
    pdf_dir = Path("/home/tahar/project/AMP/R√©glements")
    documents = []
    
    print(f"üìÇ Scan du dossier : {pdf_dir}")
    print(f"   Nombre de fichiers : {len(list(pdf_dir.glob('*.[pP][dD][fF]')))}\n")
    
    for pdf_path in pdf_dir.glob("*.[pP][dD][fF]"):  # Support .pdf et .PDF
        print(f"üîç Traitement : {pdf_path.name}")
        try:
            reader = PdfReader(str(pdf_path))
            total_text = ""
            
            for page_num, page in enumerate(reader.pages):
                text = page.extract_text() or ""
                total_text += text + "\n"
            
            # Nettoyage
            clean_text = preprocess_text(total_text)
            
            if len(clean_text) < 100:
                print(f"   ‚ö†Ô∏è  PDF vide ou scann√© (seulement {len(clean_text)} caract√®res)")
                continue
                
            documents.append(Document(
                page_content=clean_text,
                metadata={
                    "source": pdf_path.name,
                    "num_pages": len(reader.pages),
                    "char_count": len(clean_text)
                }
            ))
            
            print(f"   ‚úÖ {len(reader.pages)} pages, {len(clean_text)} caract√®res")
            
        except Exception as e:
            print(f"   ‚ùå Erreur : {e}")
    
    return documents

def build_rag_index(documents):
    """Construit l'index RAG avec chunking optimis√©."""
    print(f"\nüìä D√©coupage en chunks...")
    
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=1000,
        chunk_overlap=200,
        separators=["\n\n", "\n", ". ", " ", ""],
    )
    
    chunks = text_splitter.split_documents(documents)
    print(f"   ‚úÖ {len(chunks)} chunks cr√©√©s")
    
    # Afficher quelques statistiques
    chunk_sizes = [len(chunk.page_content) for chunk in chunks]
    print(f"   üìè Taille moyenne : {sum(chunk_sizes) // len(chunk_sizes)} caract√®res")
    print(f"   üìè Min: {min(chunk_sizes)}, Max: {max(chunk_sizes)}")
    
    print(f"\nüß† Calcul des embeddings avec all-MiniLM-L6-v2...")
    embedding_function = SentenceTransformerEmbeddings(model_name="all-MiniLM-L6-v2")
    
    print(f"üíæ Construction de l'index FAISS...")
    db = FAISS.from_documents(chunks, embedding_function)
    
    output_dir = Path("/home/tahar/project/AMP/school_assistant/data/faiss_index_new")
    output_dir.mkdir(parents=True, exist_ok=True)
    
    db.save_local(str(output_dir))
    print(f"   ‚úÖ Index sauvegard√© dans : {output_dir}")
    
    return db

def test_search(db):
    """Test de recherche."""
    print(f"\nüîç Test de recherche...")
    
    queries = [
        "Comment justifier une absence?",
        "Quels sont les horaires de cours?",
        "R√®glement informatique",
    ]
    
    for query in queries:
        print(f"\n   Q: {query}")
        docs = db.similarity_search(query, k=2)
        for i, doc in enumerate(docs, 1):
            preview = doc.page_content[:200].replace('\n', ' ')
            print(f"      {i}. [{doc.metadata['source']}] {preview}...")

if __name__ == "__main__":
    print("="*70)
    print("   TEST DE RECONSTRUCTION DE L'INDEX RAG")
    print("="*70 + "\n")
    
    # √âtape 1 : Extraction
    documents = extract_pdfs()
    
    if not documents:
        print("\n‚ùå ERREUR : Aucun document valide extrait !")
        exit(1)
    
    print(f"\nüìö Total : {len(documents)} documents extraits")
    total_chars = sum(len(doc.page_content) for doc in documents)
    print(f"   üìù {total_chars:,} caract√®res au total")
    
    # √âtape 2 : Construction RAG
    db = build_rag_index(documents)
    
    # √âtape 3 : Test
    test_search(db)
    
    print("\n" + "="*70)
    print("‚úÖ RECONSTRUCTION TERMIN√âE !")
    print("="*70)
