#!/usr/bin/env python3
"""
Chatbot RAG amÃ©liorÃ© avec support multi-LLM
- OpenAI (si clÃ© API disponible)
- Ollama (local, gratuit)
- Fallback sur recherche documentaire
"""
import os
import sys
from dotenv import load_dotenv
from typing import Optional, List
from langchain_core.documents import Document

# Imports protÃ©gÃ©s
try:
    from langchain_community.vectorstores import FAISS
    from langchain_community.embeddings import SentenceTransformerEmbeddings
except ImportError as e:
    print(f"ERREUR D'IMPORT CRITIQUE: {e}")
    print("Installez les dÃ©pendances: pip install -r requirements.txt")
    sys.exit(1)

load_dotenv()


def _format_excerpts(docs: List[Document]) -> str:
    """Formate les extraits de documents de maniÃ¨re lisible."""
    formatted = ""
    for i, doc in enumerate(docs, start=1):
        content = doc.page_content.strip()
        source = doc.metadata.get('source', 'Unknown')
        doc_type = doc.metadata.get('doc_type', 'Unknown')
        
        # AperÃ§u de 300 caractÃ¨res
        snippet = content[:300]
        if len(content) > 300:
            snippet += "â€¦"
        
        formatted += f"\nðŸ“„ **{i}. [{doc_type}] {source}**\n{snippet}\n"
    
    return formatted


def _try_groq(context: str, question: str) -> Optional[str]:
    """Tente d'utiliser Groq (prioritÃ© 1 - gratuit et rapide)."""
    api_key = os.getenv("GROQ_API_KEY")
    
    if not api_key:
        return None
    
    try:
        from langchain_groq import ChatGroq
        
        llm = ChatGroq(
            model="llama-3.3-70b-versatile",
            groq_api_key=api_key,
            temperature=0
        )
        
        prompt = f"""Tu es un assistant scolaire spÃ©cialisÃ© dans les rÃ¨glements de l'AcadÃ©mie Provinciale des MÃ©tiers (APM).

Utilise UNIQUEMENT les informations suivantes pour rÃ©pondre Ã  la question.
Si la rÃ©ponse n'est pas dans le contexte, dis-le clairement.

CONTEXTE DES RÃˆGLEMENTS :
{context}

QUESTION : {question}

RÃ‰PONSE (en franÃ§ais, claire et concise) :"""

        response = llm.invoke(prompt)
        return response.content
        
    except Exception as e:
        return None


def _try_openai(context: str, question: str) -> Optional[str]:
    """Tente d'utiliser OpenAI."""
    api_key = os.getenv("OPENAI_API_KEY")
    
    if not api_key:
        return None
    
    try:
        from langchain_openai import ChatOpenAI  # Compatible DeepSeek
        
        llm = ChatOpenAI(
            temperature=0,
            model_name="gpt-3.5-turbo",
            openai_api_key=api_key
        )
        
        prompt = f"""Tu es un assistant scolaire spÃ©cialisÃ© dans les rÃ¨glements de l'AcadÃ©mie Provinciale des MÃ©tiers (APM).

Utilise UNIQUEMENT les informations suivantes pour rÃ©pondre Ã  la question.
Si la rÃ©ponse n'est pas dans le contexte, dis-le clairement.

CONTEXTE DES RÃˆGLEMENTS :
{context}

QUESTION : {question}

RÃ‰PONSE (en franÃ§ais, claire et concise) :"""

        response = llm.invoke(prompt)
        return response.content
        
    except Exception as e:
        error_msg = str(e)
        if "insufficient_quota" in error_msg or "429" in error_msg:
            return None  # Quota Ã©puisÃ©, essayer Ollama
        raise


def _try_ollama(context: str, question: str) -> Optional[str]:
    """Tente d'utiliser Ollama."""
    try:
        from langchain_community.llms import Ollama
        
        base_url = os.getenv("OLLAMA_BASE_URL", "http://localhost:11434")
        model = os.getenv("OLLAMA_MODEL", "mistral")
        
        llm = Ollama(
            base_url=base_url,
            model=model,
            temperature=0
        )
        
        prompt = f"""Tu es un assistant scolaire spÃ©cialisÃ© dans les rÃ¨glements de l'APM.

Utilise UNIQUEMENT les informations suivantes pour rÃ©pondre.

CONTEXTE :
{context}

QUESTION : {question}

RÃ‰PONSE (franÃ§aise, concise) :"""

        response = llm.invoke(prompt)
        return response
        
    except Exception as e:
        # Ollama non installÃ© ou non dÃ©marrÃ©
        return None


def ask_bot(question: str, verbose: bool = True):
    """
    Recherche et rÃ©pond Ã  une question sur les rÃ¨glements.
    
    StratÃ©gie de fallback :
    1. OpenAI (si clÃ© valide)
    2. Ollama (si installÃ© et dÃ©marrÃ©)
    3. Recherche documentaire (toujours disponible)
    """
    base_dir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
    db_dir = os.path.join(base_dir, "data", "faiss_index")

    # 1. Charger la base FAISS
    if verbose:
        print(f"\nðŸ” Recherche pour : '{question}'")
    
    try:
        embedding_function = SentenceTransformerEmbeddings(model_name="all-MiniLM-L6-v2")
        db = FAISS.load_local(db_dir, embedding_function, allow_dangerous_deserialization=True)
    except Exception as e:
        print(f"âŒ Erreur lors du chargement de la base : {e}")
        print(f"   ExÃ©cutez d'abord : python school_assistant/chatbot/setup_rag.py")
        return

    # 2. RÃ©cupÃ©rer les documents pertinents
    retriever = db.as_retriever(search_kwargs={"k": 3})
    docs = retriever.invoke(question)
    
    if not docs:
        print("\nâš ï¸  Aucun document pertinent trouvÃ©.")
        return
    
    context = "\n\n".join([doc.page_content for doc in docs])
    
    # 3. StratÃ©gie de gÃ©nÃ©ration de rÃ©ponse
    if verbose:
        print("ðŸ’­ GÃ©nÃ©ration de la rÃ©ponse...")
    
    # Tentative 1 : Groq (prioritÃ© - gratuit et rapide)
    try:
        response = _try_groq(context, question)
        if response:
            if verbose:
                print("   âœ… Utilisation : Groq (Llama 3.3)")
            print(f"\n{'='*70}")
            print("ðŸ“ RÃ‰PONSE")
            print('='*70)
            print(response)
            print('='*70)
            return
    except Exception as e:
        if verbose:
            print(f"   âš ï¸  Groq indisponible : {e}")
    
    # Tentative 2 : OpenAI/DeepSeek
    try:
        response = _try_openai(context, question)
        if response:
            if verbose:
                print("   âœ… Utilisation : OpenAI/DeepSeek")
            print(f"\n{'='*70}")
            print("ðŸ“ RÃ‰PONSE")
            print('='*70)
            print(response)
            print('='*70)
            return
    except Exception as e:
        if verbose:
            print(f"   âš ï¸  OpenAI indisponible : {e}")
    
    # Tentative 3 : Ollama
    try:
        response = _try_ollama(context, question)
        if response:
            if verbose:
                print("   âœ… Utilisation : Ollama (local)")
            print(f"\n{'='*70}")
            print("ðŸ“ RÃ‰PONSE")
            print('='*70)
            print(response)
            print('='*70)
            return
    except Exception as e:
        if verbose:
            print(f"   â„¹ï¸  Ollama non disponible")
    
    # Fallback : Recherche documentaire
    if verbose:
        print("   â„¹ï¸  Mode : Recherche Documentaire")
        print(f"\n{'='*70}")
        print("ðŸ“š EXTRAITS PERTINENTS TROUVÃ‰S")
        print('='*70)
    
    print(_format_excerpts(docs))
    print('='*70)
    print("\nðŸ’¡ Pour une rÃ©ponse synthÃ©tisÃ©e, installez Ollama :")
    print("   curl -fsSL https://ollama.com/install.sh | sh")
    print("   ollama pull mistral")


def interactive_mode():
    """Mode interactif en ligne de commande."""
    print("\n" + "="*70)
    print("ðŸŽ“ ASSISTANT RÃˆGLEMENTS APM - Mode Interactif")
    print("="*70)
    print("Tapez 'exit' ou 'quit' pour quitter\n")
    
    while True:
        try:
            question = input("â“ Votre question : ").strip()
            
            if not question:
                continue
            
            if question.lower() in ['exit', 'quit', 'q']:
                print("\nðŸ‘‹ Au revoir !")
                break
            
            ask_bot(question)
            print()  # Ligne vide entre les questions
            
        except KeyboardInterrupt:
            print("\n\nðŸ‘‹ Au revoir !")
            break
        except Exception as e:
            print(f"\nâŒ Erreur : {e}\n")


if __name__ == "__main__":
    if len(sys.argv) > 1:
        # Mode ligne de commande
        question = " ".join(sys.argv[1:])
        ask_bot(question)
    else:
        # Mode interactif
        interactive_mode()
