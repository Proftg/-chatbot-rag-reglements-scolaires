"""
Setup RAG am√©lior√© avec embeddings multilingues et chunking intelligent.
"""
import os
import sys
from pathlib import Path

# Ajouter le chemin pour les imports
sys.path.append(str(Path(__file__).parents[1]))

from langchain_community.vectorstores import Chroma
from langchain_community.embeddings import HuggingFaceEmbeddings
from scraper.enhanced_ingest import ingest_all_pdfs
from chatbot.chunking_strategy import smart_chunk_documents
from utils.logger import setup_logger

logger = setup_logger("setup_rag")


def build_enhanced_index():
    """
    Construit un index RAG am√©lior√© avec:
    - Embeddings multilingues de qualit√©
    - Chunking intelligent selon le type de document
    - M√©tadonn√©es enrichies
    - Persistence avec ChromaDB
    """
    base_dir = Path(__file__).resolve().parents[1]
    pdf_dir = base_dir.parent / "R√©glements"
    chroma_dir = base_dir / "data" / "chroma_db_enhanced"
    
    logger.info("=" * 60)
    logger.info("CONSTRUCTION DE L'INDEX RAG AM√âLIOR√â")
    logger.info("=" * 60)
    
    # 1. Ingestion des PDFs avec m√©tadonn√©es
    logger.info("\nüì• √âtape 1: Ingestion des PDFs...")
    if not pdf_dir.exists():
        logger.error(f"‚ùå Dossier PDFs introuvable: {pdf_dir}")
        return
    
    documents = ingest_all_pdfs(pdf_dir)
    logger.info(f"‚úÖ {len(documents)} pages extraites")
    
    if not documents:
        logger.error("‚ùå Aucun document extrait. Arr√™t.")
        return
    
    # 2. Chunking intelligent
    logger.info("\n‚úÇÔ∏è  √âtape 2: D√©coupage intelligent des documents...")
    chunks = smart_chunk_documents(documents)
    logger.info(f"‚úÖ {len(chunks)} chunks cr√©√©s")
    
    # 3. Configuration des embeddings multilingues
    logger.info("\nüß† √âtape 3: Chargement du mod√®le d'embeddings multilingue...")
    logger.info("   Mod√®le: paraphrase-multilingual-mpnet-base-v2")
    logger.info("   (Optimis√© pour le fran√ßais, 768 dimensions)")
    
    embedding_function = HuggingFaceEmbeddings(
        model_name="sentence-transformers/paraphrase-multilingual-mpnet-base-v2",
        model_kwargs={'device': 'cpu'},
        encode_kwargs={'normalize_embeddings': True}
    )
    
    logger.info("‚úÖ Mod√®le charg√©")
    
    # 4. Cr√©ation de la base vectorielle ChromaDB
    logger.info(f"\nüíæ √âtape 4: Cr√©ation de la base ChromaDB...")
    logger.info(f"   Destination: {chroma_dir}")
    
    # Supprimer l'ancienne DB si elle existe
    if chroma_dir.exists():
        import shutil
        shutil.rmtree(chroma_dir)
        logger.info("   üóëÔ∏è  Ancienne DB supprim√©e")
    
    # Cr√©er la nouvelle DB avec les chunks
    db = Chroma.from_documents(
        documents=chunks,
        embedding=embedding_function,
        persist_directory=str(chroma_dir),
        collection_name="reglements_ecole",
        collection_metadata={"hnsw:space": "cosine"}
    )
    
    logger.info(f"‚úÖ Base cr√©√©e avec {len(chunks)} chunks index√©s")
    
    # 5. Validation
    logger.info("\nüîç √âtape 5: Validation de l'index...")
    test_query = "absence professeur"
    results = db.similarity_search(test_query, k=3)
    
    logger.info(f"   Test de recherche: '{test_query}'")
    logger.info(f"   R√©sultats trouv√©s: {len(results)}")
    
    if results:
        logger.info(f"   Premier r√©sultat: {results[0].metadata.get('source', 'N/A')}")
    
    logger.info("\n" + "=" * 60)
    logger.info("‚úÖ INDEX RAG AM√âLIOR√â CR√â√â AVEC SUCC√àS!")
    logger.info("=" * 60)
    logger.info(f"\nStatistiques finales:")
    logger.info(f"  - PDFs trait√©s: {len(list(pdf_dir.glob('*.pdf')))}")
    logger.info(f"  - Pages extraites: {len(documents)}")
    logger.info(f"  - Chunks index√©s: {len(chunks)}")
    logger.info(f"  - Taille moyenne chunk: {sum(len(c.page_content) for c in chunks) // len(chunks)} caract√®res")
    logger.info(f"\nüìç Base de donn√©es: {chroma_dir}")


if __name__ == "__main__":
    build_enhanced_index()
